<!DOCTYPE html><!-- Last Published: Fri Nov 08 2024 01:48:40 GMT+0000 (Coordinated Universal Time) --><html data-wf-page="634ed02c7678c4a74d8c33b4" data-wf-site="5fbc6a3c5640b420d5c9a25a"><head><meta charset="utf-8"/><title>Evaluation</title><meta content="Evaluation" property="og:title"/><meta content="Evaluation" property="twitter:title"/><meta content="width=device-width, initial-scale=1" name="viewport"/><link href="../css/webflow-style.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com" rel="preconnect"/><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous"/><script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script><script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Questrial:regular","Roboto:300,regular,500","Roboto Condensed:300,regular"]  }});</script><script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script><link href="../images/favicon.png" rel="shortcut icon" type="image/x-icon"/><link href="../images/app-icon.png" rel="apple-touch-icon"/></head><body class="body-2"><div data-collapse="medium" data-animation="default" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner" class="navbar-3 w-nav"><div class="div_studynav_holder"><a href="/study/study" class="brand-link study_home_link w-nav-brand"><img src="../images/veil_logo_w_3.png" loading="lazy" width="97" alt="Virtual  Experience Interaction Lab" class="image-10"/></a><nav role="navigation" class="nav-menu-3 studydropmenu w-nav-menu"><a href="/study/personas" class="nav_link studydrop_tabs w-nav-link">Personas</a><a href="/study/summaries" class="nav_link studydrop_tabs w-nav-link">Summaries</a><a href="/study/evaluation" aria-current="page" class="nav_link studydrop_tabs w-nav-link w--current">Evaluation‍</a><a href="/study/interaction-pattern-library-copy" class="nav_link studydrop_tabs w-nav-link">White Papers</a><a href="/study/interaction-pattern-library" class="nav_link studydrop_tabs w-nav-link">Pattern Library</a><a href="https://sites.google.com/view/veilcompanion/" target="_blank" class="nav_link studydrop_tabs w-nav-link">Knowledge Base</a></nav><div class="menu-button-2 w-nav-button"><div class="icon w-icon-nav-menu"></div></div></div></div><div id="Reasearch" class="study_section01"><div class="block_tx_bt_div"><h2 id="headIN" class="heading-3 contact-r">Evaluation</h2><p class="main-text">At the heart of our research is the complex, detailed evaluation with logic-based questions that looped depending on prior answers and pushed the limits of Qualtrics. The complex looping allows for many variations of experiences to be quantified in a similar way. It collects qualitative &amp; quantitative data and highlights different attributes of the experience including but not limited to: onboarding, navigation, locomotion, comfort / ergonomics, input (ie. controller mapping), interactions, interfaces, presence, embodiment, response from the system, narrative and inclusivity. <br/></p><div class="w-row"><div class="w-col w-col-6"><p class="caption">A screenshot of the narrative section, asking about the story elements present in the virtual experience.</p><img src="../images/narrative.png" loading="lazy" sizes="(max-width: 479px) 83vw, (max-width: 767px) 82vw, (max-width: 991px) 41vw, (max-width: 1279px) 430px, 580px" srcset="../images/narrative-p-500.png 500w, ../images/narrative-p-800.png 800w, ../images/narrative-p-1080.png 1080w, ../images/narrative.png 1294w" alt="" class="image-10 evalut_img"/></div><div class="w-col w-col-6"><p class="caption">A screenshot of the interaction patterns selection, specifically looking at the available manipulations for selection interactions.</p><img src="../images/selections.png" loading="lazy" sizes="(max-width: 479px) 83vw, (max-width: 767px) 82vw, (max-width: 991px) 41vw, (max-width: 1279px) 430px, 580px" srcset="../images/selections-p-500.png 500w, ../images/selections-p-800.png 800w, ../images/selections-p-1080.png 1080w, ../images/selections.png 1294w" alt="" class="image-10 evalut_img"/></div></div></div><section class="section-8"><h2>Evaluating Coherence</h2><p class="main-text">Our evaluation aimed to assess congruence between the objective aspects of the VR application and the subjective experience of the user. As numbers by themselves are not enough to evaluate a VR experience, we utilized a methodology that integrated qualitative and quantitative data, with the goals and characteristics of the VR experience.<br/><br/>The analysis focuses on five types of collected data: <br/></p><ol role="list"><li><strong>Narrative </strong>- What the VR experience is about and the elements displayed by it.</li><li><strong>Interactions </strong>- How the user interacts with the system.</li><li><strong>Presence </strong>- How real the user perceives the VR experience.</li><li><strong>Embodiment </strong>- How real the user perceives his or her body in the VR experience.</li><li><strong>Physiological/Affective response</strong> - Physical reactions (e.g. dizziness, heart rate, sweating) and emotions (e.g. joy, boredom).</li></ol><p class="main-text">The narrative elements and interaction modes generate a subjective perception, which incorporate the dimensions of presence, embodiment, and physiological/affective responses. This subjective perception may or may not be consistent with the aims of the VR app. We assess this consistency to indicate the overall coherence.<br/><br/>In conclusion, the narrative elements (including sounds, animations, etc.) and the interaction methods available (including menus, actions, etc.) should support the story or purpose of the VR application. Cataloging these elements is essential to assess the quality of the user experience. The levels of immersion (including presence and embodiment) and the physical and affective responses of the users will or will not be appropriate in relation to the narrative and interactive elements of the application. <br/></p></section></div><div class="section_contribute footer"><div class="w-container"><div class="columns w-row"><div class="column-2 w-col w-col-6"><a href="http://www.veilab.org" class="brand-link w-nav-brand"><img src="../images/veil_logo_w_3.png" loading="lazy" width="59" alt=""/></a><div class="footer-logo">Copyright © 2022 Virtual Experience Interaction Lab</div></div><div class="social-footer-wrap w-col w-col-6"><a href="https://medium.com/@veilab" target="_blank" class="footer-social-link w-inline-block"><img src="../images/unnamed.png" width="29" alt="Twitter" class="footer-icon"/></a><a href="https://www.youtube.com/channel/UCzRF9pw1ZhxPDAFyO6kaWbQ" target="_blank" class="footer-social-link w-inline-block"><img src="../images/395164.png" width="29" alt="Twitter" class="footer-icon"/></a><a href="https://twitter.com/VEI_Lab" target="_blank" class="footer-social-link w-inline-block"><img src="../images/twitter-icon.svg" width="29" alt="Twitter" class="footer-icon"/></a><a href="https://www.linkedin.com/company/veilab" target="_blank" class="footer-social-link w-inline-block"><img src="../images/linktinlogo_w.png" srcset="../images/linktinlogo_w-p-500.png 500w, ../images/linktinlogo_w.png 512w" sizes="100vw" width="29" alt="LinkedIn" class="footer-icon"/></a><a href="mailto:hello@veilab.org" class="footer-social-link w-inline-block"><img src="../images/mail_icon.png" width="29" alt="LinkedIn" class="footer-icon"/></a></div></div></div></div><script src="../js/jquery.js" type="text/javascript"  ></script><script src="../js/webflow-script.js" type="text/javascript"></script></body></html>